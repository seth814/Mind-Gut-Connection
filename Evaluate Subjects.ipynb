{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from models import Recurrent2DConvNet, log10\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/seth/datasets/gut'\n",
    "feats = 'stft'\n",
    "total_dur = 3\n",
    "delta_time = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(['anxiety', 'baseline', 'concentration', 'digestion', 'disgust', 'frustration'])\n",
    "n_classes = len(classes)\n",
    "int2cls = dict(zip(range(len(classes)), classes))\n",
    "cls2int = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "path = os.path.join(data_root, feats)\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for sub_dir in os.listdir(path):\n",
    "    for class_dir in os.listdir(os.path.join(path, sub_dir)):\n",
    "        cls_path = os.path.join(path, sub_dir, class_dir)\n",
    "        files = sorted(os.listdir(cls_path), key=lambda x: int(x.split('.')[0]))\n",
    "        if len(files) < total_dur:\n",
    "            continue\n",
    "        mod = len(files)%total_dur\n",
    "        orig_len = len(files)\n",
    "        for i in range(0, orig_len-mod-total_dur, total_dur):\n",
    "            paths.append(os.path.join(cls_path, files[i]))\n",
    "            labels.append(cls2int[class_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (3, 100, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "sample = np.load(paths[0])\n",
    "\n",
    "feat_dim = int(delta_time*100)\n",
    "time_dim = int(total_dur*sample.shape[0]/feat_dim)\n",
    "input_shape = (time_dim, feat_dim, sample.shape[1], 1)\n",
    "print('input shape: {}'.format(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seth/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/seth/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/seth/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 100, 128, 1)    0         \n",
      "_________________________________________________________________\n",
      "scale_input (TimeDistributed (None, 3, 100, 128, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv_block_1 (Sequential)    (None, 3, 50, 64, 8)      1248      \n",
      "_________________________________________________________________\n",
      "conv_block_2 (Sequential)    (None, 3, 25, 32, 16)     5808      \n",
      "_________________________________________________________________\n",
      "conv_block_3 (Sequential)    (None, 3, 12, 16, 32)     23136     \n",
      "_________________________________________________________________\n",
      "conv_block_4 (Sequential)    (None, 3, 6, 8, 64)       92352     \n",
      "_________________________________________________________________\n",
      "conv_block_5 (Sequential)    (None, 3, 3, 4, 128)      369024    \n",
      "_________________________________________________________________\n",
      "flatten (TimeDistributed)    (None, 3, 1536)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               852480    \n",
      "_________________________________________________________________\n",
      "dropout_0.2 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_0.1 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,348,374\n",
      "Trainable params: 1,348,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(os.path.join('models', 'RCNN.model'), custom_objects={'log10': log10})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_sub(string):\n",
    "    for _ in range(2):\n",
    "        string = os.path.split(string)[0]\n",
    "    return os.path.split(string)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_cls(string):\n",
    "    for _ in range(1):\n",
    "        string = os.path.split(string)[0]\n",
    "    return os.path.split(string)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_path(path):\n",
    "\n",
    "    base = os.path.split(path)[0]\n",
    "    start_ix = int(os.path.split(path)[-1].split('.npy')[0])\n",
    "    frames = []\n",
    "    for t in range(total_dur):\n",
    "        ix = str(start_ix + (t*100))\n",
    "        path = os.path.join(base, ix+'.npy')\n",
    "        x = np.load(path)\n",
    "        frames.append(x)\n",
    "\n",
    "    x = np.concatenate(frames, axis=0)\n",
    "\n",
    "    frames = []\n",
    "    for z in range(0, input_shape[0]*input_shape[1], input_shape[1]):\n",
    "        _slice = x[z:z+input_shape[1],:]\n",
    "        _slice = np.expand_dims(_slice, axis=0)\n",
    "        frames.append(_slice)\n",
    "    x = np.concatenate(frames, axis=0)\n",
    "    x = np.expand_dims(x, axis=3)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    y_hat = model.predict(x)\n",
    "    return np.argmax(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:09<00:00,  6.79it/s]\n"
     ]
    }
   ],
   "source": [
    "sub2data = {}\n",
    "\n",
    "for sub in tqdm(sorted(os.listdir(path))):\n",
    "    sub2data[sub] = []\n",
    "    \n",
    "    for _path in paths:\n",
    "        if sub == isolate_sub(_path):\n",
    "            cls = isolate_cls(_path)\n",
    "            sub2data[sub].append([_path, cls2int[cls]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict accuracies for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [03:12<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for sub in tqdm(sorted(os.listdir(path))):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for _path, label in sub2data[sub]:\n",
    "        y_true.append(label)\n",
    "        y_pred.append(predict_path(_path))\n",
    "        \n",
    "    acc = str(round(accuracy_score(y_true, y_pred), 4))\n",
    "    \n",
    "    results.append([sub, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['subject', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join('results', 'subject_accuracies.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a csv file in the results directory with accuracies for each subject. Just viewing accuracy isn't particularly useful in this case. Something more meaningful would be the results from cross validation. CV will allow each subject to be used in a test set exactly once, so we get an idea of how well classificaiton performs on unseen subjects.\n",
    "\n",
    "If you run python main.py and swap in train_cv(args) this will run cv for n_folds (usually 10) and save mean accuracy and standard deviation scores (train and test) for each fold. You have to move subjects you don't want trained on into another directory. I ran cross validation for both the ordered and unorder subjects. Their results are found in the pickle files. Both were trained for 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('results', 'cv_scores_p.pkl'), 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.833 +/- 0.019\n",
      " test accuracy: 0.362 +/- 0.049\n"
     ]
    }
   ],
   "source": [
    "_mean = str(round(np.mean(data['train']), 3))\n",
    "_std = str(round(np.std(data['train']), 3))\n",
    "print('train accuracy: {} +/- {}'.format(_mean, _std))\n",
    "\n",
    "_mean = str(round(np.mean(data['test']), 3))\n",
    "_std = str(round(np.std(data['test']), 3))\n",
    "print(' test accuracy: {} +/- {}'.format(_mean, _std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unordered subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('results', 'cv_scores_r.pkl'), 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.704 +/- 0.039\n",
      " test accuracy: 0.236 +/- 0.069\n"
     ]
    }
   ],
   "source": [
    "_mean = str(round(np.mean(data['train']), 3))\n",
    "_std = str(round(np.std(data['train']), 3))\n",
    "print('train accuracy: {} +/- {}'.format(_mean, _std))\n",
    "\n",
    "_mean = str(round(np.mean(data['test']), 3))\n",
    "_std = str(round(np.std(data['test']), 3))\n",
    "print(' test accuracy: {} +/- {}'.format(_mean, _std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons I can think of why test accuracies might be lower than train accuracies.\n",
    "\n",
    "1. Bowl sounds are unique and 50 subjects is not enough to estimate a population.\n",
    "2. Systematic error is being introduced in regards to stethoscope placement on the belly. Sounds can be coming from different places in the intestine, so one side might capture entirely different data from the other. This might cause an inconsistent measure of bowl sounds such that each patient is very unique and the true source of data is hidden from us.\n",
    "\n",
    "The good thing here is that test accuracy is poor for both ordered and unordered subjects. So it seems classification isn't entirely biased due to a decreasing voltage from the sensor over time like we thought. Also, the drop in accuracy between the two sets does suggest that an increase in data size might help. (meaning we are not capturing a population and gut sounds are unique).\n",
    "\n",
    "Another thing to mention is the accuracy around 80% from the model results notebook. It seems there are real differences between mental states and gut sounds. As long as the stethoscope's position wasn't moved between activities, then it's possible a mind gut connection does exist to some degree; however, this is not certain since generalization to new data seems to fail in the cross validation. It is also possible that events of interest which cause class separation are sparse. This might be able to be detected by using a larger total duration of time. This could lead to a higher degree of overfitting, but it is worth looking into. The main issue would be the input size into the network will start to consume too much memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
